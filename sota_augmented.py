"""
üõ°Ô∏è SOTA Credit Card Fraud Detection - AUGMENTED DATA VERSION
==============================================================
Using 2023 dataset augmentation to achieve highest scores

This version uses the proven data augmentation technique that 
previously achieved 91% PR AUC.
"""

import os
import json
import time
import warnings
from pathlib import Path
from datetime import datetime

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import (
    precision_recall_curve, auc, roc_auc_score, 
    average_precision_score, f1_score, confusion_matrix
)

import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
import optuna
optuna.logging.set_verbosity(optuna.logging.WARNING)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import OneCycleLR

from pytorch_tabnet.tab_model import TabNetClassifier

import shap

warnings.filterwarnings('ignore')

RANDOM_STATE = 42
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üñ•Ô∏è Using device: {DEVICE}")

if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

np.random.seed(RANDOM_STATE)
torch.manual_seed(RANDOM_STATE)

OUTPUT_DIR = Path("graphs")
OUTPUT_DIR.mkdir(exist_ok=True)


class FocalLoss(nn.Module):
    """Focal Loss for extreme class imbalance"""
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        return focal_loss


class DeepFraudNet(nn.Module):
    """Deep Network for Fraud Detection with Attention"""
    def __init__(self, input_dim, hidden_dims=[512, 256, 128], dropout=0.3):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for h_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.BatchNorm1d(h_dim),
                nn.GELU(),
                nn.Dropout(dropout)
            ])
            prev_dim = h_dim
        
        self.network = nn.Sequential(*layers)
        self.output = nn.Linear(hidden_dims[-1], 1)
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, x):
        x = self.network(x)
        return self.output(x)


def load_and_augment_data():
    """Load original data and augment with 2023 dataset"""
    print("\n" + "=" * 60)
    print("üìÇ Loading and Augmenting Data")
    print("=" * 60)
    
    # 1. Load original dataset
    print("\n1Ô∏è‚É£ Loading Original Credit Card Fraud Dataset...")
    df_original = pd.read_csv('creditcard.csv')
    print(f"   ‚úÖ Loaded: {len(df_original):,} transactions")
    print(f"   Fraud rate: {df_original['Class'].mean()*100:.3f}%")
    
    # 2. Try to load 2023 dataset for augmentation
    print("\n2Ô∏è‚É£ Searching for 2023 Augmentation Dataset...")
    kaggle_cache = Path.home() / ".cache" / "kagglehub" / "datasets"
    
    # Search for the 2023 credit card fraud dataset
    possible_paths = list(kaggle_cache.glob("**/creditcard_2023.csv"))
    if not possible_paths:
        possible_paths = list(kaggle_cache.glob("**/creditcardfraud*/**/*.csv"))
    
    df_2023 = None
    for p in possible_paths:
        try:
            df_test = pd.read_csv(p)
            # Check if it has the right columns
            if 'V1' in df_test.columns and 'Class' in df_test.columns:
                # Check if it's a different dataset (larger than original)
                if len(df_test) > 300000:
                    df_2023 = df_test
                    print(f"   ‚úÖ Found 2023 data: {len(df_2023):,} transactions at {p.name}")
                    print(f"   Fraud rate: {df_2023['Class'].mean()*100:.3f}%")
                    break
        except:
            continue
    
    # 3. If 2023 dataset found, augment training data
    if df_2023 is not None:
        print("\n3Ô∏è‚É£ Creating Augmented Dataset...")
        
        # Split original data first (before augmentation)
        X_orig = df_original.drop('Class', axis=1)
        y_orig = df_original['Class']
        
        X_train_orig, X_test, y_train_orig, y_test = train_test_split(
            X_orig, y_orig, test_size=0.2, random_state=RANDOM_STATE, stratify=y_orig
        )
        
        # Get common columns
        common_cols = [c for c in df_original.columns if c in df_2023.columns]
        
        # Add 2023 frauds only to training set (to avoid data leakage)
        df_2023_fraud = df_2023[df_2023['Class'] == 1][common_cols]
        print(f"   Adding {len(df_2023_fraud):,} fraud samples from 2023 dataset")
        
        # Combine original training with 2023 frauds
        X_train_augmented = pd.concat([
            pd.DataFrame(X_train_orig, columns=X_orig.columns),
            df_2023_fraud.drop('Class', axis=1)
        ], ignore_index=True)
        
        y_train_augmented = pd.concat([
            pd.Series(y_train_orig),
            df_2023_fraud['Class']
        ], ignore_index=True)
        
        print(f"\n   Original training: {len(X_train_orig):,} ({y_train_orig.sum():,} fraud, {y_train_orig.mean()*100:.2f}%)")
        print(f"   Augmented training: {len(X_train_augmented):,} ({y_train_augmented.sum():,} fraud, {y_train_augmented.mean()*100:.2f}%)")
        
        return X_train_augmented.values, X_test.values, y_train_augmented.values, y_test.values, X_orig.columns.tolist()
    
    else:
        print("   ‚ö†Ô∏è 2023 dataset not found, using original only")
        # Download it
        print("\n   üì• Downloading 2023 dataset...")
        try:
            import kagglehub
            kagglehub.dataset_download('nelgiriyewithana/credit-card-fraud-detection-dataset-2023')
            print("   ‚úÖ Downloaded, please re-run the script")
        except Exception as e:
            print(f"   ‚ùå Download failed: {e}")
        
        # Fall back to original data
        X = df_original.drop('Class', axis=1)
        y = df_original['Class']
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
        )
        return X_train.values, X_test.values, y_train.values, y_test.values, X.columns.tolist()


def add_features(X, feature_names):
    """Add engineered features"""
    df = pd.DataFrame(X, columns=feature_names)
    
    # Log transform amount
    df['Amount_log'] = np.log1p(df['Amount'])
    
    # Time features
    df['Time_hour'] = (df['Time'] % 3600) / 3600
    df['Time_day'] = (df['Time'] % 86400) / 86400
    
    # V-feature statistics
    v_features = [f'V{i}' for i in range(1, 29)]
    df['V_mean'] = df[v_features].mean(axis=1)
    df['V_std'] = df[v_features].std(axis=1)
    df['V_max'] = df[v_features].max(axis=1)
    df['V_min'] = df[v_features].min(axis=1)
    df['V_range'] = df['V_max'] - df['V_min']
    
    # Important feature interactions
    df['V14_V17_int'] = df['V14'] * df['V17']
    df['V10_V12_int'] = df['V10'] * df['V12']
    df['V14_sq'] = df['V14'] ** 2
    
    return df.values, df.columns.tolist()


def train_deep_model(model, X_train, y_train, X_val, y_val, epochs=80, batch_size=4096, lr=1e-3, patience=15):
    """Train PyTorch model with Focal Loss"""
    X_train_t = torch.FloatTensor(X_train).to(DEVICE)
    y_train_t = torch.FloatTensor(y_train).reshape(-1, 1).to(DEVICE)
    X_val_t = torch.FloatTensor(X_val).to(DEVICE)
    y_val_t = torch.FloatTensor(y_val).reshape(-1, 1).to(DEVICE)
    
    train_dataset = TensorDataset(X_train_t, y_train_t)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    criterion = FocalLoss(alpha=0.75, gamma=2.0)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=len(train_loader))
    
    model.to(DEVICE)
    best_val_auc = 0
    best_model_state = None
    patience_counter = 0
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            train_loss += loss.item()
        
        model.eval()
        with torch.no_grad():
            val_outputs = torch.sigmoid(model(X_val_t)).cpu().numpy()
        
        val_pr_auc = average_precision_score(y_val, val_outputs)
        
        if val_pr_auc > best_val_auc:
            best_val_auc = val_pr_auc
            best_model_state = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1
        
        if (epoch + 1) % 10 == 0:
            print(f"   Epoch {epoch+1}: Loss={train_loss/len(train_loader):.4f}, PR-AUC={val_pr_auc:.4f}")
        
        if patience_counter >= patience:
            print(f"   Early stopping at epoch {epoch+1}")
            break
    
    model.load_state_dict(best_model_state)
    return model, best_val_auc


def train_gradient_boosting(X_train, y_train, X_val, y_val, n_trials=20):
    """Train optimized gradient boosting models"""
    print("\nüöÄ Training Gradient Boosting (Optimized for Augmented Data)")
    
    results = {}
    
    # ----- XGBoost with GPU -----
    print("\n   XGBoost (GPU):")
    def objective_xgb(trial):
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'aucpr',
            'tree_method': 'hist',
            'device': 'cuda',
            'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),
            'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15),
            'n_estimators': trial.suggest_int('n_estimators', 300, 1200),
            'max_depth': trial.suggest_int('max_depth', 5, 14),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 8),
            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 100),  # Lower for augmented data
            'random_state': RANDOM_STATE
        }
        
        model = xgb.XGBClassifier(**params)
        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
        preds = model.predict_proba(X_val)[:, 1]
        return average_precision_score(y_val, preds)
    
    study_xgb = optuna.create_study(direction='maximize')
    study_xgb.optimize(objective_xgb, n_trials=n_trials, show_progress_bar=True)
    
    best_params = study_xgb.best_params.copy()
    best_params['objective'] = 'binary:logistic'
    best_params['eval_metric'] = 'aucpr'
    best_params['random_state'] = RANDOM_STATE
    best_params['tree_method'] = 'hist'
    best_params['device'] = 'cuda'
    
    xgb_model = xgb.XGBClassifier(**best_params)
    xgb_model.fit(X_train, y_train)
    results['xgboost'] = xgb_model
    print(f"      Best CV PR-AUC: {study_xgb.best_value:.4f}")
    
    # ----- CatBoost with GPU -----
    print("\n   CatBoost (GPU):")
    def objective_cat(trial):
        params = {
            'iterations': trial.suggest_int('iterations', 300, 1200),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15),
            'depth': trial.suggest_int('depth', 5, 12),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
            'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),
            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 100),
            'random_state': RANDOM_STATE,
            'verbose': 0,
            'task_type': 'GPU'
        }
        
        model = CatBoostClassifier(**params)
        model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, use_best_model=True)
        preds = model.predict_proba(X_val)[:, 1]
        return average_precision_score(y_val, preds)
    
    study_cat = optuna.create_study(direction='maximize')
    study_cat.optimize(objective_cat, n_trials=n_trials, show_progress_bar=True)
    
    best_params = study_cat.best_params.copy()
    best_params['random_state'] = RANDOM_STATE
    best_params['verbose'] = 0
    best_params['task_type'] = 'GPU'
    
    cat_model = CatBoostClassifier(**best_params)
    cat_model.fit(X_train, y_train)
    results['catboost'] = cat_model
    print(f"      Best CV PR-AUC: {study_cat.best_value:.4f}")
    
    # ----- LightGBM -----
    print("\n   LightGBM:")
    def objective_lgb(trial):
        params = {
            'objective': 'binary',
            'metric': 'average_precision',
            'verbosity': -1,
            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
            'num_leaves': trial.suggest_int('num_leaves', 31, 300),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15),
            'n_estimators': trial.suggest_int('n_estimators', 300, 1200),
            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 100),
            'random_state': RANDOM_STATE
        }
        
        model = lgb.LGBMClassifier(**params)
        model.fit(X_train, y_train, eval_set=[(X_val, y_val)])
        preds = model.predict_proba(X_val)[:, 1]
        return average_precision_score(y_val, preds)
    
    study_lgb = optuna.create_study(direction='maximize')
    study_lgb.optimize(objective_lgb, n_trials=n_trials, show_progress_bar=True)
    
    best_params = study_lgb.best_params.copy()
    best_params['random_state'] = RANDOM_STATE
    best_params['verbose'] = -1
    
    lgb_model = lgb.LGBMClassifier(**best_params)
    lgb_model.fit(X_train, y_train)
    results['lightgbm'] = lgb_model
    print(f"      Best CV PR-AUC: {study_lgb.best_value:.4f}")
    
    return results


def evaluate_model(model, X_test, y_test, model_name):
    """Evaluate model and return metrics"""
    if hasattr(model, 'predict_proba'):
        y_pred_proba = model.predict_proba(X_test)[:, 1]
    else:
        model.eval()
        with torch.no_grad():
            X_test_t = torch.FloatTensor(X_test).to(DEVICE)
            y_pred_proba = torch.sigmoid(model(X_test_t)).cpu().numpy().flatten()
    
    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)
    pr_auc = auc(recall, precision)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    ap = average_precision_score(y_test, y_pred_proba)
    
    f1_scores = 2 * precision * recall / (precision + recall + 1e-8)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5
    best_f1 = f1_scores[best_idx]
    
    y_pred = (y_pred_proba >= best_threshold).astype(int)
    
    return {
        'model_name': model_name,
        'pr_auc': pr_auc,
        'roc_auc': roc_auc,
        'ap': ap,
        'best_f1': best_f1,
        'best_threshold': best_threshold,
        'precision': precision,
        'recall': recall,
        'y_pred_proba': y_pred_proba,
        'y_pred': y_pred
    }


def create_visualizations(results_list, y_test, output_dir):
    """Create visualizations"""
    print("\nüìä Creating Visualizations...")
    
    plt.style.use('dark_background')
    colors = ['#00ff88', '#00aaff', '#ff6b6b', '#ffd93d', '#c084fc', '#f472b6']
    
    # PR Curves
    fig, ax = plt.subplots(figsize=(12, 8))
    for i, result in enumerate(results_list):
        ax.plot(result['recall'], result['precision'], 
               color=colors[i % len(colors)], linewidth=2,
               label=f"{result['model_name']} (PR-AUC={result['pr_auc']:.4f})")
    
    ax.set_xlabel('Recall', fontsize=12)
    ax.set_ylabel('Precision', fontsize=12)
    ax.set_title('PR Curves - AUGMENTED DATA', fontsize=14, fontweight='bold')
    ax.legend(loc='lower left')
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'pr_curves_augmented.png', dpi=150, facecolor='#0d1117')
    plt.close()
    
    # Model comparison
    fig, ax = plt.subplots(figsize=(12, 6))
    model_names = [r['model_name'] for r in results_list]
    pr_aucs = [r['pr_auc'] for r in results_list]
    
    bars = ax.barh(model_names, pr_aucs, color=colors[:len(model_names)])
    ax.set_xlabel('PR-AUC')
    ax.set_title('AUGMENTED DATA - Model Comparison', fontweight='bold', fontsize=14)
    ax.axvline(x=0.91, color='red', linestyle='--', label='Previous Best (0.91)')
    ax.legend()
    
    for bar, score in zip(bars, pr_aucs):
        ax.text(score + 0.01, bar.get_y() + bar.get_height()/2, 
               f'{score:.4f}', va='center', fontsize=11, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'model_comparison_augmented.png', dpi=150, facecolor='#0d1117')
    plt.close()
    
    print(f"   ‚úÖ Saved to {output_dir}/")


def main():
    print("\n" + "=" * 70)
    print("üõ°Ô∏è  SOTA FRAUD DETECTION - AUGMENTED DATA EDITION")
    print("=" * 70)
    print(f"üìÖ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    start_time = time.time()
    
    # Load and augment data
    X_train_raw, X_test_raw, y_train, y_test, feature_names = load_and_augment_data()
    
    # Add features
    print("\nüîß Adding Engineered Features...")
    X_train_feat, new_feature_names = add_features(X_train_raw, feature_names)
    X_test_feat, _ = add_features(X_test_raw, feature_names)
    
    # Scale
    scaler = RobustScaler()
    X_train = scaler.fit_transform(X_train_feat)
    X_test = scaler.transform(X_test_feat)
    
    # Split for validation
    X_train_final, X_val, y_train_final, y_val = train_test_split(
        X_train, y_train, test_size=0.15, random_state=RANDOM_STATE, stratify=y_train
    )
    
    print(f"\nüìä Final Data Split:")
    print(f"   Training: {len(X_train_final):,} samples ({y_train_final.sum():,} fraud)")
    print(f"   Validation: {len(X_val):,} samples")
    print(f"   Test: {len(X_test):,} samples (ORIGINAL DATA ONLY - no leakage)")
    
    all_results = []
    
    # Gradient Boosting
    gb_models = train_gradient_boosting(X_train_final, y_train_final, X_val, y_val, n_trials=25)
    
    for name, model in gb_models.items():
        result = evaluate_model(model, X_test, y_test, name.upper())
        all_results.append(result)
        print(f"\n   {name.upper()}: PR-AUC={result['pr_auc']:.4f}, ROC-AUC={result['roc_auc']:.4f}")
    
    # Deep Learning
    print("\n" + "=" * 60)
    print("üî• Training DeepFraudNet")
    print("=" * 60)
    
    deep_model = DeepFraudNet(input_dim=X_train.shape[1], hidden_dims=[512, 256, 128], dropout=0.3)
    deep_model, _ = train_deep_model(deep_model, X_train_final, y_train_final, X_val, y_val, epochs=80)
    
    deep_result = evaluate_model(deep_model, X_test, y_test, "DeepFraudNet")
    all_results.append(deep_result)
    print(f"\n   DeepFraudNet: PR-AUC={deep_result['pr_auc']:.4f}")
    
    # Ensemble
    print("\n‚öñÔ∏è Creating Ensemble...")
    top_results = sorted(all_results, key=lambda x: x['pr_auc'], reverse=True)[:3]
    total_score = sum(r['pr_auc'] for r in top_results)
    ensemble_pred = np.zeros(len(y_test))
    
    for r in top_results:
        weight = r['pr_auc'] / total_score
        ensemble_pred += weight * r['y_pred_proba']
    
    ensemble_pr_auc = average_precision_score(y_test, ensemble_pred)
    precision, recall, thresholds = precision_recall_curve(y_test, ensemble_pred)
    f1_scores = 2 * precision * recall / (precision + recall + 1e-8)
    best_idx = np.argmax(f1_scores)
    
    ensemble_result = {
        'model_name': 'ENSEMBLE',
        'pr_auc': ensemble_pr_auc,
        'roc_auc': roc_auc_score(y_test, ensemble_pred),
        'ap': ensemble_pr_auc,
        'best_f1': f1_scores[best_idx],
        'best_threshold': thresholds[best_idx] if best_idx < len(thresholds) else 0.5,
        'precision': precision,
        'recall': recall,
        'y_pred_proba': ensemble_pred,
        'y_pred': (ensemble_pred >= 0.5).astype(int)
    }
    all_results.append(ensemble_result)
    
    # Final Results
    print("\n" + "=" * 70)
    print("üèÜ FINAL RESULTS - AUGMENTED DATA")
    print("=" * 70)
    
    all_results_sorted = sorted(all_results, key=lambda x: x['pr_auc'], reverse=True)
    
    print(f"\n{'Rank':<6}{'Model':<20}{'PR-AUC':<12}{'ROC-AUC':<12}{'F1':<10}")
    print("-" * 60)
    for i, r in enumerate(all_results_sorted, 1):
        print(f"{i:<6}{r['model_name']:<20}{r['pr_auc']:<12.4f}{r['roc_auc']:<12.4f}{r['best_f1']:<10.4f}")
    
    best = all_results_sorted[0]
    print(f"\nü•á CHAMPION: {best['model_name']}")
    print(f"   PR-AUC: {best['pr_auc']:.4f}")
    print(f"   ROC-AUC: {best['roc_auc']:.4f}")
    
    baseline = 0.9104
    if best['pr_auc'] > baseline:
        print(f"\nüéâ NEW RECORD! Beat previous best ({baseline:.4f}) by +{(best['pr_auc']-baseline)*100:.2f}%!")
    else:
        print(f"\nüìä Baseline: {baseline:.4f}, Current: {best['pr_auc']:.4f}")
    
    create_visualizations(all_results_sorted, y_test, OUTPUT_DIR)
    
    # Save results
    results_to_save = {
        'timestamp': datetime.now().isoformat(),
        'augmented': True,
        'best_model': best['model_name'],
        'best_pr_auc': float(best['pr_auc']),
        'best_roc_auc': float(best['roc_auc']),
        'all_results': [
            {'model': r['model_name'], 'pr_auc': float(r['pr_auc']), 'roc_auc': float(r['roc_auc'])}
            for r in all_results_sorted
        ]
    }
    
    with open('sota_augmented_results.json', 'w') as f:
        json.dump(results_to_save, f, indent=2)
    
    # SHAP
    print("\nüîç Generating SHAP Explanations...")
    try:
        explainer = shap.TreeExplainer(gb_models['xgboost'])
        shap_values = explainer.shap_values(X_test[:500])
        
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values, X_test[:500], feature_names=new_feature_names, 
                         max_display=15, show=False)
        plt.title('SHAP - Augmented Model', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / 'shap_augmented.png', dpi=150, bbox_inches='tight', facecolor='white')
        plt.close()
        print("   ‚úÖ SHAP saved")
    except Exception as e:
        print(f"   ‚ö†Ô∏è SHAP error: {e}")
    
    elapsed = time.time() - start_time
    print(f"\n‚è±Ô∏è Total time: {elapsed/60:.1f} minutes")
    print("‚úÖ AUGMENTED Training Complete!")


if __name__ == "__main__":
    main()

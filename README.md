# ğŸ›¡ï¸ Credit Card Fraud Detection

> ğŸ“Š **Inspired by:** [Credit Card Fraud Detection Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
>
> Machine learning project achieving **91% PR AUC** for detecting fraudulent transactions using ensemble methods, deep learning, and data augmentation.

ğŸ”— **[View Live Dashboard](https://ericdataplus.github.io/credit-card-fraud-detect/)**

![Model Comparison](graphs/pr_curves_comparison.png)

## ğŸ“Š Key Results

| Metric | Value |
|--------|-------|
| Best PR AUC | **91.04%** (Augmented) |
| Best on Original Data | **87.91%** (XGBoost) |
| Dataset Size | 284,807 transactions |
| Fraud Rate | 0.17% |
| SOTA Models | 7 |
| GPU Used | RTX 3060 12GB |

## ğŸ† December 2025 SOTA Benchmark

| Model | PR AUC | ROC AUC | F1 | Dataset |
|-------|--------|---------|-----|---------|
| **XGBoost** | **87.91%** | 97.43% | 88.9% | Original |
| CatBoost | 87.20% | 96.92% | 89.5% | Original |
| Ensemble | 87.10% | 96.21% | 88.5% | Original |
| LightGBM | 86.75% | 97.25% | 87.0% | Original |
| TabTransformer | 73.72% | 96.09% | 78.7% | Original |
| TabNet | 73.37% | 98.18% | 77.0% | Original |
| DeepFraudNet | 68.72% | 97.90% | 80.2% | Original |
| **Augmented XGBoost** | **91.04%** | - | - | +2023 Data |

## ğŸ” Key Findings

1. **Data Augmentation = +3.3% Boost** â€” Adding external 2023 fraud data increased training fraud rate from 0.17% to 55.6%
2. **XGBoost Beats Deep Learning** â€” Gradient boosting (88-91%) outperformed neural networks (68-74%) on structured tabular data
3. **No Data Leakage** â€” Rigorous integrity check confirmed 0 duplicates between augmented data and test set
4. **Beats Academic Baselines** â€” Our 87.91% exceeds typical research benchmarks of 85-86%
5. **SHAP Explainability** â€” V14, V17, V12, V10 are the most important fraud indicators

## ğŸ§  SOTA Techniques Implemented

### Deep Learning (GPU)
- **TabNet** - Google's attention-based tree mimic
- **DeepFraudNet** - Custom architecture with Focal Loss
- **TabularTransformer** - Feature tokenization + transformer encoder

### Gradient Boosting (GPU-Accelerated)
- **XGBoost** with Optuna hyperparameter tuning (25 trials)
- **CatBoost** GPU training
- **LightGBM** ensemble

### Advanced Features
- **Focal Loss** for extreme class imbalance (Î±=0.75, Î³=2.0)
- **SHAP Explainability** for model interpretability
- **Weighted Ensemble** combining top models

## ğŸ“ Project Structure

```
credit-card-fraud-detect/
â”œâ”€â”€ index.html                    # Interactive Dashboard
â”œâ”€â”€ graphs/                       # Visualizations
â”‚   â”œâ”€â”€ pr_curves_comparison.png  # All models PR curves
â”‚   â”œâ”€â”€ shap_summary.png          # SHAP feature importance
â”‚   â”œâ”€â”€ model_comparison.png      # Bar chart comparison
â”‚   â””â”€â”€ confusion_matrix_best.png
â”œâ”€â”€ sota_fraud_detection.py       # SOTA training script (GPU)
â”œâ”€â”€ sota_augmented.py             # Augmented data training
â”œâ”€â”€ sota_results.json             # Benchmark results
â”œâ”€â”€ predict_fraud.py              # Production prediction script
â”œâ”€â”€ best_xgboost_model.json       # Saved best model
â”œâ”€â”€ deep_fraud_net.pt             # PyTorch DeepFraudNet
â”œâ”€â”€ tab_transformer.pt            # PyTorch Transformer
â””â”€â”€ creditcard.csv                # Dataset (not in repo)
```

## ğŸ› ï¸ Tech Stack

- **Python** - Core language
- **XGBoost / CatBoost / LightGBM** - Gradient boosting
- **PyTorch** - Deep learning (TabNet, Transformer, Focal Loss)
- **Optuna** - Hyperparameter optimization
- **SHAP** - Model explainability
- **Scikit-learn** - Preprocessing & metrics

## ğŸ“¦ Data Sources

- Primary: [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) - 284K transactions
- Augmentation: [Credit Card Fraud 2023](https://www.kaggle.com/datasets/nelgiriyewithana/credit-card-fraud-detection-dataset-2023) - 568K transactions
- Bank Fraud: [NeurIPS 2022](https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022) - 1M accounts
- PaySim: [Mobile Money](https://www.kaggle.com/datasets/ealaxi/paysim1) - 6.3M transactions

## ğŸš€ Quick Start

```bash
# Clone the repo
git clone https://github.com/Ericdataplus/credit-card-fraud-detect.git
cd credit-card-fraud-detect

# Install dependencies
pip install -r requirements.txt

# Download dataset from Kaggle and place creditcard.csv in root

# Run SOTA training (requires GPU)
python sota_fraud_detection.py

# Or run prediction on new data
python predict_fraud.py
```

## ğŸ“ˆ Why PR AUC?

For **highly imbalanced datasets** (0.17% fraud), accuracy is misleading. A model predicting "not fraud" always gets 99.83% accuracy!

**Precision-Recall AUC** measures:
- **Precision**: Of flagged transactions, how many are actually fraud?
- **Recall**: Of all frauds, how many did we catch?

This is the industry standard for fraud detection.

## ğŸ’° Business Impact

To deploy commercially, tune the decision threshold based on:
- **False Negative Cost**: Lost transaction + chargeback fees (~$180 average)
- **False Positive Cost**: Lost sale + customer friction (~$40)

At optimal threshold, catching 85% of fraud while only flagging 5% false positives can save millions annually.

---

Made with ğŸ›¡ï¸ by [Ericdataplus](https://github.com/Ericdataplus) | December 2025 (SOTA Update)